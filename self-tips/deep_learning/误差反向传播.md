# 线性模型的表示

$$
y=\sum_i{w_ix_i} + b
$$

##  线性模型前项传播的计算公式

$$
a^{(1)}=xW^{(1)}, y=a^{(1)}W^{{2}}
$$

$$
y=x(W^{(1)})W^{(2)}=x(W^{{1}}W^{(2)})=xW'
$$

从这个公式可以看出，虽然神经网络有两层，但它和单层的神经网络的表达能力没有什么不同。以此类推，只通过线性变换，任意层的全连接神经网络和单层神经网络模型的表达能力没有任何区别，而且它们都是线性模型。

# 激活函数实现去线性化

最终输出的时候，增加一层激活函数，实现了神经网络模型的非线性。
$$
A_1=[a_{1},a_{2},a_{3}]=f(xW^{(1)} + b)
\\=f([x_1,x_2]\left[ \begin{matrix} w_{1,1}^{(1)} &w_{1,2}^{(1)} &w_{1,3}^{(1)} \\ w_{2,1}^{(1)} &w_{2,2}^{(1)} &w_{2,3}^{(1)}  \end{matrix} \right] + \left[\begin{matrix} b_1 &b_2 &b_3 \end{matrix}]\right)
\\=[\begin{matrix}f(w_{{1,1}}^{(1)}x_1+w_{2,1}^{(1)}x_2+b_1),&f(w_{{1,2}}^{(1)}x_1+w_{2,2}^{(1)}x_2+b_2),&f(w_{{1,3}}^{(1)}x_1+w_{2,3}^{(1)}x_2+b_3) \end{matrix}]
$$
激活函数$f()$一般有

ReLU函数：$f(x)=max(x,0)$

sigmoid函数： $f(x)=\frac 1 {1+e^{-x}}$

tanh函数： $\frac {1-e^{-2x}} {1+e^{-2x}}$



令

$$
z_{1}=w_{{1,1}}^{(1)}x_1+w_{2,1}^{(1)}x_2+b_1
$$

则
$$
a_{11}=f(z_{1})
$$

# 梯度下降的基本思路

对于函数 $y=f(wx+b)$

$w$是权重。训练时是已知$(x,y)$的pair对的，主要是训练$w$和$b$

找极值点，可以给初值$x_0$,然后$x_{k+1} = x_k - \eta\dot{f}(x)$

样本输出为$(x_k, y_k)$,网络输出为$\hat{y}_k$,则输出误差的平方差可表示为
$$
c_k=(y_k-\hat{y}_k)^2
$$


则系数的偏导可表示为：
$$
(\Delta{w_{1,1}}, \Delta{w_{1,2}},\Delta{w_{1,3}},\Delta{b_{1}},\Delta{b_{2}},\Delta{b_{3}}) =
-\eta(\frac{\partial{c1}} {\partial{w_{1,1}}}, \frac{\partial{c2}} {\partial{w_{1,2}}}, \frac{\partial{c3}} {\partial{w_{1,3}}},\frac{\partial{c1}} {\partial{b_1}},\frac{\partial{c1}} {\partial{b_2}},\frac{\partial{c1}} {\partial{b_3}})
$$


梯度下降的核心在于求误差对于系数的偏导数。

# 误差反向传播

## 单层神经网络神经单元误差

对于单层神经网络，它的第$k$个输出误差为$c_{k}$,则第$k$个神经元的神经元单元误差为
$$
\delta_k=\frac{\partial{c_k}} {\partial{z_k}}
$$
其中C为误差的集合。$z_k=w_{1,k}^{(1)}x_1 + w_{2,k}^{(1)}x_2+b_k$
$$
y_k=f(z_k)
$$

$$
c_k=(y_k-\hat{y}_k)^2
$$

$$
\frac {\partial{c_k}} {\partial{z_k}}=\frac {\partial{c_k}} {\partial{y_k}} \cdot \frac {\partial{y_k}} {\partial{z_k}}=\frac {\partial{c_k}} {\partial{y_k}} \cdot \dot{f}(z_k)=2(y_k-\hat{y}_k) \cdot \dot{f}(z_k)
$$

则
$$
\delta_k=2(y_k-\hat{y}_k) \cdot \dot{f}(z_k)
$$
其中$y_k$为实际观测到的值，$\hat{y}_k$为以$w$为参数模型算出来的值，$\dot{f}$为激活函数的导数。对于选取到的合理的激活函数，激活函数的导数相对容易。

则
$$
\frac {\partial{c_k}} {\partial{w_{1,k}}}=\frac {\partial{c_k}} {\partial{z_{k}}} \frac {\partial{z_k}} {\partial{w_{1,k}}}=\delta_k \cdot x_1 \\
\frac {\partial{c_k}} {\partial{b_{k}}}=\delta_k
$$
即算出了最后一层误差对于$w_{1,k}$的偏导数。

## 多层神经网络的神经单元误差

对于多层神经网络，设第$l$层的第k个神经单元误差为$\delta_k^{(l)}$,前一层神经网络输出值的加权和为$z_k^{(l)}$,输出为$y_k^{(l)}$，$y_k^{(l)}=f(z_k^{(l)})$

对于$z_k^{(l+1)}$有，前一层的隐藏层共有$n$个单元
$$
z_k^{(l+1)}=w_{{1,k}}^{(l)}y_1^{(l)}+w_{2,k}^{(1)}y_2^{(l)}+\cdots \cdots +b_k^{(l)}=\sum_{i=1\to n}w_{i,k}^{(l)}y_k^{(l)} + b_k^{(l)}
$$
且
$$
\delta_k^{(l+1)}=\frac {\partial{C}} {\partial{z_k^{(l+1)}}}
$$

$$
\delta_k^{(l)}=\frac {\partial{C}} {\partial{z_k^{(l)}}}
=\frac {\partial{C}} {\partial{z_{1}^{(l+1)}}} \cdot \frac {\partial{z_{1}^{(l+1)}}} {\partial{z_{1}^{(l)}}} +
\frac {\partial{C}} {\partial{z_{2}^{(l+1)}}} \cdot \frac {\partial{z_{2}^{(l+1)}}} {\partial{z_{2}^{(l)}}} +
\cdots \cdots +
=\frac {\partial{C}} {\partial{z_{n}^{(l+1)}}} \cdot \frac {\partial{z_{n}^{(l+1)}}} {\partial{z_{n}^{(l)}}}\\
=\sum_{i=1 \to n} {(\frac {\partial{C}} {\partial{z_i^{(l+1)}}} \cdot \frac {\partial{z_i^{(l+1)}}} {\partial {z_k^{(l)}}} )} \\
=\sum_{i=i \to n}{(\delta_i^{(l+1)} \cdot w_{i,k} \cdot \frac {\partial{y_k^{(l)}}} {\partial{z_k^{(l)}}}) }
\\=(\sum_{i=i \to n}{\delta_i^{(l+1)} \cdot w_{i,k} ) \cdot \dot{f}(z_k^{(l)})}
$$

则输出误差相对于$w_{i,k}$的偏层数
$$
\frac {\partial {C}} {\partial{w_{i,k}^{(l)}}}
= \frac {\partial {C}} {\partial{z_k^{(l)}}} \cdot \frac {\partial {z_k^{(l)}}} {\partial{w_{i,k}}}
= \delta_k^{(l)} \cdot y_i^{(l-1)}
$$

$$
\frac {\partial{C}} {\partial{b_k^{(l)}}}=\delta_k^{(l)}
$$



则隐藏层的误差可通过后一层的误差推导。从而实现了隐藏层误差的链式计算。